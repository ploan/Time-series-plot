{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b90351-d5e5-4d58-a7b7-52dec9b45cfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Master Utility Notebook for Lisa's plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8149f404-39cb-47af-ab92-e05c6a9452bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Used in Conjunction with other notebooks for analysis and plotting** <br>\n",
    "Phuong Loan Nguyen et al. 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3502f169-4890-4dba-afaa-3d10e05d1de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules \n",
    "import pandas as pd\n",
    "import os\n",
    "import fnmatch\n",
    "import xarray as xr\n",
    "import xskillscore as xs\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfebe26-e130-4cb9-be83-b2275c796620",
   "metadata": {
    "tags": []
   },
   "source": [
    "## File Input Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7685453-5431-4db1-9531-6b1553360838",
   "metadata": {
    "tags": []
   },
   "source": [
    "To get data from different directories within my CMIP5 database, f-strings are used in the analysis notebooks. <br>\n",
    "**This will likely need to be updated based on your database structure** <br>\n",
    "- The **CMIP6** database is set-up so that the Climpact indices follow the path: <br>\n",
    "    - parent_directory/{indice-keyword}/{ANN-or-MON}/dataset_file.nc <br><br>\n",
    "- The **observational** dataset (APHRODITE) is organized differently in the database: <br>\n",
    "    - parent_directory/CMIP6/{obs}/{grid_type}/{climpact-or-daily-data}/{indice-keyword}/dataset_file_MON-or-ANN.nc\n",
    "\n",
    "**Functions Include:**\n",
    "- Get data model names from dictionaries below\n",
    "- Get subset of models from dictionary defined in analysis notebooks\n",
    "- Get model data paths\n",
    "- Get data from paths\n",
    "    - This includes exceptions to handle the 4-dimensionality of the SPI variable and the \"day\" unit of the CD(W)D and R_mm variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363de8cc-1d91-4bc6-8912-013aeee9e019",
   "metadata": {},
   "source": [
    "### Define names of models\n",
    "I select models from my database based on the GCM and RCM names included in the file name. This will likely need to be updated for use in other databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ae749d-297c-4595-b6aa-fe3033f49134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List names of forcing GCMs\n",
    "gcm_names = ['ACCESS-CM2', 'ACCESS-ESM1-5', 'BCC-CSM2-MR', 'CNRM-CM6-1', 'CNRM-CM6-1-HR', 'CNRM-ESM2-1', 'EC-Earth3-Veg', \n",
    "            'FGOALS-g3', 'GFDL-CM4', 'GFDL-ESM4', 'HadGEM3-GC31-LL', 'HadGEM3-GC31-MM', 'INM-CM4-8', 'INM-CM5-0', 'KACE-1-0-G',\n",
    "    'KIOST-ESM', 'MIROC-ES2L', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NESM3', 'NorESM2-LM', 'NorESM2-MM', 'UKESM1-0-LL', 'CanESM5', 'CanESM5', 'MPI-ESM1-2-LR', \n",
    "            'CPC_v1.0', 'GPCC_FDD_v2022', 'REGEN_ALL_2019', '3B42_v7.0','GIRAFE', 'GSMAP-NRT-gauges-v8.0', 'IMERG-v07B-FC', 'PERSIANN_v1_r1', \n",
    "             'CHIRPS_v2.0','GPCP_V3.2', 'CMORPH_v1.0_CRT', 'MERRA2','JRA-55', 'ERA5', 'CFSR'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f920104-de8c-440d-9e76-c63bb0e43910",
   "metadata": {},
   "source": [
    "### Get Model Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c1ede1-1dcf-48cf-ac49-33da0d0def2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dictionaries above to get the model names from each file\n",
    "def get_data_name(names, file):\n",
    "    \n",
    "    # Check the model name is in one of the above model lists\n",
    "    for name in names:\n",
    "        if fnmatch.fnmatch(file, \"*\" + name + \"_*\"):\n",
    "            return name\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28383372-f50d-4de8-b831-99dcc86d4ade",
   "metadata": {},
   "source": [
    "### Get Model Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d224a5-2905-4836-82ca-1d98ddfc5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model file paths and add to a Pandas Dataframe \n",
    "def get_model_files(model_data_master_path):\n",
    "    \n",
    "    # Initialise pandas df of model_paths\n",
    "    model_paths = pd.DataFrame(columns=['driving_gcm', 'dataset_path'])\n",
    "\n",
    "    # Loop through the data directory passed to the function, check that the file is a model in the above lists, and get the dataset paths\n",
    "    for model_file in os.listdir(model_data_master_path):\n",
    "    \n",
    "        # Get the RCM and GCM files names\n",
    "        driving_gcm_name = get_data_name(gcm_names, model_file)\n",
    "    \n",
    "        # Get full path to the dataset \n",
    "        model_complete_file_path = model_data_master_path + model_file\n",
    "            \n",
    "        # Add information to DataFrame\n",
    "        model_paths.loc[len(model_paths.index)] = [driving_gcm_name, model_complete_file_path]\n",
    "            \n",
    "    return model_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ca6f6-61f2-4fa1-82e8-6eca48b0cd12",
   "metadata": {},
   "source": [
    "### Get Model Data Paths for Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf5a8b-7bc4-40f4-b05b-bace12272bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model file paths for subset of models from list defined above\n",
    "def get_model_files_subset(model_paths, subset_names):\n",
    "    \n",
    "    # Initialize new Pandas Dataframe to hold data paths for subsets of models\n",
    "    model_paths_subset = pd.DataFrame(columns=['model_name', 'dataset_path'])\n",
    "\n",
    "    for i, row in model_paths.iterrows():\n",
    "        model_name = f'{row[0]}'\n",
    "    \n",
    "        if model_name in subset_names:\n",
    "            model_paths_subset.loc[len(model_paths_subset.index)] = [model_name, f'{row[1]}']\n",
    "        \n",
    "    return model_paths_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333256c0-ad00-4354-9ba7-04da53e711b9",
   "metadata": {},
   "source": [
    "### Get Data from File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53d5654-dd01-431c-8b8a-70b10d62ad65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract data from files based on user specifications \n",
    "def get_data_from_file(file_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None):\n",
    "    \n",
    "    # If iscale is not given, check other variable exceptions before extracting data\n",
    "    if iscale is None:\n",
    "    \n",
    "        # If the variable is CDD/CWD (Consecutive Dry/Wet Days) or R*mm (number of heavy rain days), the data must be read in differently to convert the variable dtype to float\n",
    "        if variable == 'cdd':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.cdd.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.cdd.attrs['units'] = 'days'\n",
    "            data_var = data_ds.cdd.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "            \n",
    "        elif variable == 'cwd':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.cwd.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.cwd.attrs['units'] = 'days'\n",
    "            data_var = data_ds.cwd.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "    \n",
    "        elif variable == 'r10mm':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.r10mm.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.r10mm.attrs['units'] = 'days'\n",
    "            data_var = data_ds.r10mm.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "        \n",
    "        elif variable == 'r20mm':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.r20mm.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.r20mm.attrs['units'] = 'days'\n",
    "            data_var = data_ds.r20mm.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "        \n",
    "        elif variable == 'r30mm':\n",
    "        \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.r30mm.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.r30mm.attrs['units'] = 'days'\n",
    "            data_var = data_ds.r30mm.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "            \n",
    "        elif variable == 'fracprday':\n",
    "            \n",
    "            data_ds = xr.open_dataset(file_path, decode_cf=False)\n",
    "            data_ds.fracprday.attrs['units'] = '1'\n",
    "            data_ds = xr.decode_cf(data_ds)\n",
    "            data_ds.fracprday.attrs['units'] = 'days'\n",
    "            data_var = data_ds.fracprday.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "            \n",
    "    \n",
    "        else:\n",
    "    \n",
    "            # Open file and store data as a DataArray\n",
    "            data_ds = xr.open_dataset(file_path)\n",
    "\n",
    "            # Subset data based on user-specified spatiotemporal boundaries\n",
    "            data_subset = data_ds.sel(time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "    \n",
    "            # Extract user-specified variable\n",
    "        \n",
    "            data_var = getattr(data_subset, variable)\n",
    "   \n",
    "    # Extract appropriate SPI data based on the user-defined scale if the iscale is not None\n",
    "    # (1 for 3-month averaging period, 2 for 6-month averaging period, 3 for 12-month averaging period)\n",
    "    else:\n",
    "        \n",
    "        # Extract user-specified SPI data\n",
    "        data_ds = xr.open_dataset(file_path)\n",
    "        data_var = data_ds.spi.sel(scale=iscale, time=time_slice, lat=lat_slice, lon=lon_slice)\n",
    "        \n",
    "    if season is None: \n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        data_var = data_var.sel(time=data_var.time.dt.month.isin(season))\n",
    "    \n",
    "    return data_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28793a30-eb6b-4c91-b766-da944b2742c6",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c8ff0-5eb9-4f45-ae67-f45bc5de2396",
   "metadata": {},
   "source": [
    "Functions used when creating plots/figures. <br>\n",
    "**Functions Include:** <br>\n",
    "- Truncating a predefined colormap\n",
    "- Define vertical bands to shade along a time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac1853-8240-4238-8f58-ec69cc201908",
   "metadata": {},
   "source": [
    "### Truncate a color map (skip colors in pre-defined colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "178092ba-0b9f-46b9-b386-7c2ef6bcfec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to be able to skip colors/subset colors within a predefined colormap\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "# Code sourced from https://stackoverflow.com/questions/18926031/how-to-extract-a-subset-of-a-colormap-as-a-new-colormap-in-matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8356b-5437-4f93-a8b1-ff56edc0e521",
   "metadata": {},
   "source": [
    "### Define vertical bands (regions) to shade along a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "606d96fb-eee2-4398-936c-85b2456e85d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define function to find areas that need to be shaded using a boolean mask as input\n",
    "def fill_vertical_columns(boolean_fill_mask):\n",
    "    \n",
    "    # Find areas in the mask when values change (i.e. boolean switched from True to False and vice versa)\n",
    "    boolean_switch = np.diff(boolean_fill_mask)\n",
    "    \n",
    "    # Find start and end of sections where the boolean fill mask is True (places I want to shade)\n",
    "    region_to_shade, = boolean_switch.nonzero()\n",
    "    \n",
    "    # Handle edge cases where condition starts or ends with True\n",
    "    if boolean_fill_mask[0]:\n",
    "        region_to_shade = np.r_[0, region_to_shade]\n",
    "   \n",
    "    if boolean_fill_mask[-1]:\n",
    "        region_to_shade = np.r_[region_to_shade, len(boolean_fill_mask)]\n",
    "    \n",
    "    # Reshape the result into pairs of start/end indices\n",
    "    region_to_shade = region_to_shade.reshape((-1, 2))\n",
    "    \n",
    "    return region_to_shade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf016c-5231-4f30-8e9f-fa2548cd8ad1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spatially Averaged Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b0b78-bb7a-47da-8f80-37dc754fef56",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following functions calculate spatially averaged metrics, primarily for plotting time series and scatterplots. <br>\n",
    "'season=None' sets an optional argument for seasonal subsetting based on numeric month values <br>\n",
    "'mask=None' sets an optional argument for masking, where the default is None <br>\n",
    "'scale=None' sets an optional argument for selecting the scale of the 4-D SPI indice. This should only be assigned a value <br>\n",
    "    if the variable is SPI. The scale can be 3, 6, or 12 months. <br>\n",
    "**Functions Include:** <br>\n",
    "- Annual Time Series\n",
    "- Weighted Spatial Average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c35117-8461-450f-a118-e8ddd5006953",
   "metadata": {},
   "source": [
    "### Weighted Spatial Mean at Default Time Step (No Temporal Averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b1b3d2-eb4e-44d7-8733-42ed66e348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get monthly averages to use for a line plot over a user-specified time period and spatial domain\n",
    "def get_weighted_ts(data_path, variable, time_slice, lat_slice, lon_slice, season=None, iscale=None, mask=None):\n",
    "    \n",
    "    # Get data\n",
    "    # Include SPI scale in data extraction if a scale is provided by the user\n",
    "    if iscale is None:\n",
    "        data = get_data_from_file(data_path, variable, time_slice, lat_slice, lon_slice, season)\n",
    "    \n",
    "    else:\n",
    "        data = get_data_from_file(data_path, variable, time_slice, lat_slice, lon_slice, season, iscale)\n",
    "              \n",
    "    # Extract unmasked data if no mask provided\n",
    "    if mask is None:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Apply mask to data\n",
    "        data = data.where(mask==1)\n",
    "    \n",
    "    # Compute latitudinally weighted spatially averaged data\n",
    "    # Create latitudinal weights\n",
    "    weights = np.cos(np.deg2rad(data.lat))\n",
    "    weights.name = \"weights\"\n",
    "\n",
    "    # Weight: apply weighted lats to data\n",
    "    data_weighted = data.weighted(weights)\n",
    "\n",
    "    # Get weighted averages at each time step\n",
    "    data_weighted_mean = data_weighted.mean(dim=('lon', 'lat'))\n",
    "    \n",
    "    \n",
    "    # Return weighted spatial average at default time step\n",
    "    return data_weighted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e2826f-56c1-4e15-8fb5-bd2b29fe9c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clim_labs",
   "language": "python",
   "name": "clim_labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
